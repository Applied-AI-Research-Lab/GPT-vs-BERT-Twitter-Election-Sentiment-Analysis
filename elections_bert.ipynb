{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maVTL5fR52-g"
      },
      "source": [
        "* Upload files to GDrive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxSzIcYd5ymz"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# for fn in uploaded.keys():\n",
        "#   print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKyd142Myc0u"
      },
      "source": [
        "*   To handle files from Google Drive, Colab should have the necessary permissions to mount the drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7DY7DHYvync"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# # Give permissions to colab handling files from gdrive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# # Check if the file exists\n",
        "# import os\n",
        "# file_path = '/content/gdrive/My Drive/SentimentBERT/dataset-test.csv'\n",
        "# if os.path.exists(file_path):\n",
        "#     print(\"File exists.\")\n",
        "# else:\n",
        "#     print(\"File does not exist.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEzDQ1lUy_uj"
      },
      "source": [
        "* Use the training dataset uploaded to Google Drive to **train** the **BERT** model using **AdamW Optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dk4ioPNpgY9j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8865b82b-db85-4631-d767-7e54813c2d2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Epoch 1/3 (Training): 100%|██████████| 175/175 [00:12<00:00, 14.27it/s]\n",
            "Epoch 1/3 (Validation): 100%|██████████| 38/38 [00:01<00:00, 31.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3 - Training Loss: 0.4379, Validation Loss: 0.8982, Validation Accuracy: 0.5933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 (Training): 100%|██████████| 175/175 [00:12<00:00, 14.34it/s]\n",
            "Epoch 2/3 (Validation): 100%|██████████| 38/38 [00:01<00:00, 32.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3 - Training Loss: 0.4033, Validation Loss: 0.8923, Validation Accuracy: 0.6233\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 (Training): 100%|██████████| 175/175 [00:12<00:00, 14.37it/s]\n",
            "Epoch 3/3 (Validation): 100%|██████████| 38/38 [00:01<00:00, 33.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3 - Training Loss: 0.3177, Validation Loss: 0.8628, Validation Accuracy: 0.6300\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/gdrive/My Drive/ElectionsComments/Datasets/BERT/label_encoder.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive # Use Google classes\n",
        "\n",
        "drive.mount('/content/gdrive') # Mount Google Drive using Google Colab to access files stored in Google Drive\n",
        "\n",
        "# Load the datasets\n",
        "train_path = \"/content/gdrive/My Drive/ElectionsComments/Datasets/train_dataset.csv\"\n",
        "val_path = '/content/gdrive/My Drive/ElectionsComments/Datasets/validation_dataset.csv'\n",
        "\n",
        "train_data = pd.read_csv(train_path)\n",
        "val_data = pd.read_csv(val_path)\n",
        "\n",
        "# Encode the sentiment labels\n",
        "label_encoder = LabelEncoder()\n",
        "train_data['sentiment'] = label_encoder.fit_transform(train_data['sentiment'])\n",
        "val_data['sentiment'] = label_encoder.transform(val_data['sentiment'])\n",
        "\n",
        "# Define a custom dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        comment = str(self.data.comment[index])\n",
        "        sentiment = self.data.sentiment[index]\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            comment,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=True,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'comment': comment,\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'token_type_ids': inputs['token_type_ids'].flatten(),\n",
        "            'sentiment': torch.tensor(sentiment, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Set up the BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_))\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "max_len = 128\n",
        "train_dataset = CustomDataset(train_data, tokenizer, max_len)\n",
        "val_dataset = CustomDataset(val_data, tokenizer, max_len)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# Set up optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_dataloader) * 3  # 3 epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "# Training loop with validation\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for batch in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{epochs} (Training)'):\n",
        "        inputs = {\n",
        "            'input_ids': batch['input_ids'].to(device),\n",
        "            'attention_mask': batch['attention_mask'].to(device),\n",
        "            'token_type_ids': batch['token_type_ids'].to(device),\n",
        "            'labels': batch['sentiment'].to(device)\n",
        "        }\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_dataloader, desc=f'Epoch {epoch + 1}/{epochs} (Validation)'):\n",
        "            inputs = {\n",
        "                'input_ids': batch['input_ids'].to(device),\n",
        "                'attention_mask': batch['attention_mask'].to(device),\n",
        "                'token_type_ids': batch['token_type_ids'].to(device),\n",
        "                'labels': batch['sentiment'].to(device)\n",
        "            }\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            predictions = torch.argmax(logits, dim=1)\n",
        "            val_correct += (predictions == batch['sentiment'].to(device)).sum().item()\n",
        "\n",
        "    val_accuracy = val_correct / len(val_dataset)\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{epochs} - Training Loss: {loss.item():.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "# Save the fine-tuned model and tokenizer\n",
        "output_dir = '/content/gdrive/My Drive/ElectionsComments/Datasets/BERT/adamw'\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Save the label encoder\n",
        "label_encoder_path = '/content/gdrive/My Drive/ElectionsComments/Datasets/BERT/label_encoder.pkl'\n",
        "joblib.dump(label_encoder, label_encoder_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85WSnVAHzcgP"
      },
      "source": [
        "* Use the trained **BERT** model (**AdamW Optimizer**) to **predict** ratings for the **test** dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UASLNi6vrgy",
        "outputId": "6c2bc176-822b-4e7e-fbcc-14d1eb8f2600"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Making Predictions: 100%|██████████| 38/38 [02:16<00:00,  3.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               comment  bert_predictions\n",
            "0      over 1000 doctors appeal modi enforce ecigs ban                 0\n",
            "1    what saying you are with modi you are chutiya ...                 1\n",
            "2    blog mantri backmodi jumlebaazrahul not modi b...                 1\n",
            "3    why dont fight from varanasi modi left his hom...                 0\n",
            "4    dear hes not just like you varun dhawan others...                 1\n",
            "..                                                 ...               ...\n",
            "295                        modi phir seso vote for bjp                 0\n",
            "296                        india needs modi government                 0\n",
            "297  our beloved prime minister modi will the prime...                 1\n",
            "298  but election dates get nearer bjp support dwin...                 1\n",
            "299  wife sri modi the first family definition corr...                 1\n",
            "\n",
            "[300 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import joblib\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model_dir = '/content/gdrive/My Drive/ElectionsComments/Datasets/BERT/adamw'\n",
        "model = BertForSequenceClassification.from_pretrained(model_dir)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
        "\n",
        "# Load the label encoder\n",
        "label_encoder_path = '/content/gdrive/My Drive/ElectionsComments/Datasets/BERT/label_encoder.pkl'\n",
        "label_encoder = joblib.load(label_encoder_path)\n",
        "\n",
        "# Load data to make predictions on (replace with your dataset)\n",
        "data_path = \"/content/gdrive/My Drive/ElectionsComments/Datasets/test_dataset.csv\"\n",
        "data = pd.read_csv(data_path)\n",
        "\n",
        "# Tokenize and prepare inputs for the model\n",
        "tokenized_inputs = tokenizer(data['comment'].tolist(), add_special_tokens=True, max_length=128, padding='max_length', return_tensors='pt')\n",
        "\n",
        "# Make predictions\n",
        "model.eval()\n",
        "predicted_sentiments = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in tqdm(range(0, len(data), 8), desc='Making Predictions'):\n",
        "        batch_inputs = {key: val[i:i+8] for key, val in tokenized_inputs.items()}\n",
        "\n",
        "        outputs = model(**batch_inputs)\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "        # Decode predicted labels using label encoder\n",
        "        batch_predictions = label_encoder.inverse_transform(predictions.cpu().numpy())\n",
        "        predicted_sentiments.extend(batch_predictions)\n",
        "\n",
        "# Add predictions to the original DataFrame\n",
        "data['bert_predictions'] = predicted_sentiments\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_combined_file = '/content/gdrive/My Drive/ElectionsComments/Datasets/BERT/combined_test_predictions.csv'\n",
        "data.to_csv(output_combined_file, index=False)\n",
        "\n",
        "# Display predictions\n",
        "print(data[['comment', 'bert_predictions']])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}